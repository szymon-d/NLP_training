{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    sent = nltk.sent_tokenize(text = text)\n",
    "    corpus = ''\n",
    "    for i in sent:\n",
    "        review = re.sub('/s+', ' ', i)\n",
    "        review = re.sub('/d', '', review)\n",
    "        review = re.sub(',', '', review)\n",
    "        tokens = nltk.word_tokenize(review)\n",
    "        tokens = ' '.join(tokens)\n",
    "        corpus += tokens\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = text_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_duplicate(text):\n",
    "    tokens = text.split(' ')\n",
    "    tokens = set(tokens)\n",
    "    corpus = ' '.join(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = del_duplicate(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.With .My vague bell fashion while has deeply introduce drug saw in suits readers did mental .â€śWedlock which memory akin summons twice disturbing observing position Thursday abhorrent blind I .â€śIt under sensitive daily .It were admirable .He throw mind eagerly there ; take attracted inside results scent sufficient emotions their false up those for Baker said trained they careless .Beyond old told Holland cold threw rubbed abandoned admit .I more admirably well-remembered â€ť precise felt observerâ€ faculties happiness to however lately drowsiness save love : armchair passed â€śmy word things led temperament one would returned hardly observation fail given gasogene crack lit .They pass on Irene never scored been first leather came put thought nervous shared Holmes rise yourself week Sherlock from dubious merely harness.â€ť almost home fancy questionable high-power 1888â€ trifle by have pounds deduce cigars as press successfully walk together .One nature with name sunk certainly menâ€™s silhouette returning ambition ” wet He Watson ( attention than six pacing instrument delicate all mess case kindly â€śMy seldom might notice much but lenses doubt Scarlet clasped new an around passions form remarked reasoning spirit .All man way practice predominates long eclipses Trincomalee himself through wooing figure shoe any itselfâ€ť girl some machine him .â€śIndeed yet knew home-centred dear strikes ever official rooms corner .Just twentieth habit waved ? glad looked servant Jane tell caused excellent finally getting must friend introspective mission head imagine against alternating fierce room rang before brothers employing time her tall finds you â€śthis March who effusive upon door .To journey Bohemian emotion fire account Odessa the eye out.â€ť always it spoke Adler extraordinary heard delicately tragedy know a mysteries veil country immense whole seven signs finely my every sex half Street society since be intended perfect woman .And among indicated spoken marriage drifted .How associated youâ€ť clearing out .But keen Study Mary chuckled reigning energy distracting how she centuries again Holmesâ€ť late us seized risen clothes not absorb Trepoff civil most â€śI books .Obviously away accomplished cocaine few ago his true factor motives .From hopeless your problem powers attitude answered crime our cuts behind adjusted â€śSeven or is burned remained former singular incidents occupied canâ€™t observe just too companion .You nightâ€ eyes .â€śI hands and of chamber study between into changed lover so loathed chest reasoner was little spare where seen sneer .As shown very see murder desire parallel hot such â€śThen incorrigible simplicity world should across .In brilliantly .Grit that lodgings now ) softer .His buried complete do gibe dark had strong story swiftly clues own activity mood master over ! dreadful think doings manner stood family placed each left when interests intrusions .Then dreams lived even firelight drawing part actions soul clumsy police at drug-created other wife these mention work formerly balanced patient go following he particularly you.â€ť me still establishment Atkinson'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "features_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text_3])\n",
    "\n",
    "tokenizer_rep = tokenizer.texts_to_sequences([text_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data):\n",
    "    data = np.squeeze(np.array(data))\n",
    "    X_len = 3\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i > len(data) - X_len - 1:\n",
    "            continue\n",
    "        else:\n",
    "            X.append(data[i:i + X_len])\n",
    "            y.append(data[i + X_len])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_split(tokenizer_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, features_num, input_length = 3))\n",
    "model.add(LSTM(units = 1000, return_sequences = True))\n",
    "model.add(LSTM(units = 1000))\n",
    "model.add(Dense(units = 1000, activation = 'relu'))\n",
    "model.add(Dense(units = vocab_size, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 3, 10)             10000     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 3, 1000)           4044000   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 1000)              8004000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1000)              1001000   \n",
      "=================================================================\n",
      "Total params: 14,060,000\n",
      "Trainable params: 14,060,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "8/8 [==============================] - 27s 1s/step - loss: 6.9084\n",
      "Epoch 2/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 6.8867\n",
      "Epoch 3/60\n",
      "8/8 [==============================] - 13s 2s/step - loss: 6.5820\n",
      "Epoch 4/60\n",
      "8/8 [==============================] - 12s 1s/step - loss: 6.2704\n",
      "Epoch 5/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 6.1737\n",
      "Epoch 6/60\n",
      "8/8 [==============================] - 13s 2s/step - loss: 6.1249\n",
      "Epoch 7/60\n",
      "8/8 [==============================] - 8s 997ms/step - loss: 6.1202\n",
      "Epoch 8/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 6.1075\n",
      "Epoch 9/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 6.0744\n",
      "Epoch 10/60\n",
      "8/8 [==============================] - 8s 954ms/step - loss: 6.0724\n",
      "Epoch 11/60\n",
      "8/8 [==============================] - 8s 967ms/step - loss: 6.0443\n",
      "Epoch 12/60\n",
      "8/8 [==============================] - 7s 798ms/step - loss: 5.9689\n",
      "Epoch 13/60\n",
      "8/8 [==============================] - 7s 900ms/step - loss: 5.9671\n",
      "Epoch 14/60\n",
      "8/8 [==============================] - 8s 940ms/step - loss: 5.9218\n",
      "Epoch 15/60\n",
      "8/8 [==============================] - 7s 876ms/step - loss: 5.8477\n",
      "Epoch 16/60\n",
      "8/8 [==============================] - 7s 871ms/step - loss: 5.8977\n",
      "Epoch 17/60\n",
      "8/8 [==============================] - 7s 796ms/step - loss: 5.7940\n",
      "Epoch 18/60\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 5.6366\n",
      "Epoch 19/60\n",
      "8/8 [==============================] - 7s 813ms/step - loss: 5.4466\n",
      "Epoch 20/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 5.3892\n",
      "Epoch 21/60\n",
      "8/8 [==============================] - 8s 927ms/step - loss: 5.3218\n",
      "Epoch 22/60\n",
      "8/8 [==============================] - 7s 818ms/step - loss: 5.2261\n",
      "Epoch 23/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 5.1017\n",
      "Epoch 24/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 4.9517\n",
      "Epoch 25/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 4.8625\n",
      "Epoch 26/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 4.8185\n",
      "Epoch 27/60\n",
      "8/8 [==============================] - 8s 940ms/step - loss: 4.6968\n",
      "Epoch 28/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 4.6193\n",
      "Epoch 29/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 4.5655\n",
      "Epoch 30/60\n",
      "8/8 [==============================] - 12s 2s/step - loss: 4.4521\n",
      "Epoch 31/60\n",
      "8/8 [==============================] - 8s 978ms/step - loss: 4.3162\n",
      "Epoch 32/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 4.2603\n",
      "Epoch 33/60\n",
      "8/8 [==============================] - 7s 887ms/step - loss: 4.0072\n",
      "Epoch 34/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.7787\n",
      "Epoch 35/60\n",
      "8/8 [==============================] - 8s 915ms/step - loss: 3.5729\n",
      "Epoch 36/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.4662\n",
      "Epoch 37/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 3.2232\n",
      "Epoch 38/60\n",
      "8/8 [==============================] - 7s 862ms/step - loss: 3.0005\n",
      "Epoch 39/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.8285\n",
      "Epoch 40/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 2.4070\n",
      "Epoch 41/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.1186\n",
      "Epoch 42/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 2.0114\n",
      "Epoch 43/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.9113\n",
      "Epoch 44/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.8083\n",
      "Epoch 45/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.7870\n",
      "Epoch 46/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.7681\n",
      "Epoch 47/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 1.7630\n",
      "Epoch 48/60\n",
      "8/8 [==============================] - 8s 1s/step - loss: 1.7152\n",
      "Epoch 49/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.5473\n",
      "Epoch 50/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.5544\n",
      "Epoch 51/60\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.4391\n",
      "Epoch 52/60\n",
      "8/8 [==============================] - 8s 989ms/step - loss: 1.1803\n",
      "Epoch 53/60\n",
      "8/8 [==============================] - 8s 969ms/step - loss: 1.0681\n",
      "Epoch 54/60\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.9984\n",
      "Epoch 55/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.9525\n",
      "Epoch 56/60\n",
      "8/8 [==============================] - 8s 993ms/step - loss: 0.9777\n",
      "Epoch 57/60\n",
      "8/8 [==============================] - 8s 976ms/step - loss: 0.9566\n",
      "Epoch 58/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.9986\n",
      "Epoch 59/60\n",
      "8/8 [==============================] - 8s 943ms/step - loss: 0.9050\n",
      "Epoch 60/60\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x162b3ec7ac0>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 60, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for key in tokenizer.word_index.keys():\n",
    "    word_index.update({tokenizer.word_index[key]: key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    y_pred = model.predict(tokenized_text)\n",
    "    y_pred = y_pred.argmax()\n",
    "    \n",
    "    word = word_index[y_pred]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Why','have', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    to_pred = ' '.join(text[len(text)-3:])\n",
    "    word = predict(to_pred)\n",
    "    text.append(word)\n",
    "\n",
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why have you over loathed â€śi police part journey created itselfâ€ť dreadful excellent high six did away came answered rooms saw â€śi akin be true true harness from from home holmes holmes strikes official few admit â€śi true introspective sherlock home canâ€™t books'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€ť 1\n",
      "he 2\n",
      "you 3\n",
      "with 4\n",
      "my 5\n",
      "drug 6\n",
      "in 7\n",
      "i 8\n",
      "it 9\n",
      "they 10\n",
      "to 11\n",
      "â€śmy 12\n",
      "one 13\n",
      "from 14\n",
      "home 15\n",
      "as 16\n",
      "all 17\n",
      "but 18\n",
      "just 19\n",
      "out 20\n",
      "and 21\n",
      "how 22\n",
      "study 23\n",
      "â€śi 24\n",
      "his 25\n",
      "vague 26\n",
      "bell 27\n",
      "fashion 28\n",
      "while 29\n",
      "has 30\n",
      "deeply 31\n",
      "introduce 32\n",
      "saw 33\n",
      "suits 34\n",
      "readers 35\n",
      "did 36\n",
      "mental 37\n",
      "â€śwedlock 38\n",
      "which 39\n",
      "memory 40\n",
      "akin 41\n",
      "summons 42\n",
      "twice 43\n",
      "disturbing 44\n",
      "observing 45\n",
      "position 46\n",
      "thursday 47\n",
      "abhorrent 48\n",
      "blind 49\n",
      "â€śit 50\n",
      "under 51\n",
      "sensitive 52\n",
      "daily 53\n",
      "were 54\n",
      "admirable 55\n",
      "throw 56\n",
      "mind 57\n",
      "eagerly 58\n",
      "there 59\n",
      "take 60\n",
      "attracted 61\n",
      "inside 62\n",
      "results 63\n",
      "scent 64\n",
      "sufficient 65\n",
      "emotions 66\n",
      "their 67\n",
      "false 68\n",
      "up 69\n",
      "those 70\n",
      "for 71\n",
      "baker 72\n",
      "said 73\n",
      "trained 74\n",
      "careless 75\n",
      "beyond 76\n",
      "old 77\n",
      "told 78\n",
      "holland 79\n",
      "cold 80\n",
      "threw 81\n",
      "rubbed 82\n",
      "abandoned 83\n",
      "admit 84\n",
      "more 85\n",
      "admirably 86\n",
      "well 87\n",
      "remembered 88\n",
      "precise 89\n",
      "felt 90\n",
      "observerâ€ 91\n",
      "faculties 92\n",
      "happiness 93\n",
      "however 94\n",
      "lately 95\n",
      "drowsiness 96\n",
      "save 97\n",
      "love 98\n",
      "armchair 99\n",
      "passed 100\n",
      "word 101\n",
      "things 102\n",
      "led 103\n",
      "temperament 104\n",
      "would 105\n",
      "returned 106\n",
      "hardly 107\n",
      "observation 108\n",
      "fail 109\n",
      "given 110\n",
      "gasogene 111\n",
      "crack 112\n",
      "lit 113\n",
      "pass 114\n",
      "on 115\n",
      "irene 116\n",
      "never 117\n",
      "scored 118\n",
      "been 119\n",
      "first 120\n",
      "leather 121\n",
      "came 122\n",
      "put 123\n",
      "thought 124\n",
      "nervous 125\n",
      "shared 126\n",
      "holmes 127\n",
      "rise 128\n",
      "yourself 129\n",
      "week 130\n",
      "sherlock 131\n",
      "dubious 132\n",
      "merely 133\n",
      "harness 134\n",
      "almost 135\n",
      "fancy 136\n",
      "questionable 137\n",
      "high 138\n",
      "power 139\n",
      "1888â€ 140\n",
      "trifle 141\n",
      "by 142\n",
      "have 143\n",
      "pounds 144\n",
      "deduce 145\n",
      "cigars 146\n",
      "press 147\n",
      "successfully 148\n",
      "walk 149\n",
      "together 150\n",
      "nature 151\n",
      "name 152\n",
      "sunk 153\n",
      "certainly 154\n",
      "menâ€™s 155\n",
      "silhouette 156\n",
      "returning 157\n",
      "ambition 158\n",
      "” 159\n",
      "wet 160\n",
      "watson 161\n",
      "attention 162\n",
      "than 163\n",
      "six 164\n",
      "pacing 165\n",
      "instrument 166\n",
      "delicate 167\n",
      "mess 168\n",
      "case 169\n",
      "kindly 170\n",
      "seldom 171\n",
      "might 172\n",
      "notice 173\n",
      "much 174\n",
      "lenses 175\n",
      "doubt 176\n",
      "scarlet 177\n",
      "clasped 178\n",
      "new 179\n",
      "an 180\n",
      "around 181\n",
      "passions 182\n",
      "form 183\n",
      "remarked 184\n",
      "reasoning 185\n",
      "spirit 186\n",
      "man 187\n",
      "way 188\n",
      "practice 189\n",
      "predominates 190\n",
      "long 191\n",
      "eclipses 192\n",
      "trincomalee 193\n",
      "himself 194\n",
      "through 195\n",
      "wooing 196\n",
      "figure 197\n",
      "shoe 198\n",
      "any 199\n",
      "itselfâ€ť 200\n",
      "girl 201\n",
      "some 202\n",
      "machine 203\n",
      "him 204\n",
      "â€śindeed 205\n",
      "yet 206\n",
      "knew 207\n",
      "centred 208\n",
      "dear 209\n",
      "strikes 210\n",
      "ever 211\n",
      "official 212\n",
      "rooms 213\n",
      "corner 214\n",
      "twentieth 215\n",
      "habit 216\n",
      "waved 217\n",
      "glad 218\n",
      "looked 219\n",
      "servant 220\n",
      "jane 221\n",
      "tell 222\n",
      "caused 223\n",
      "excellent 224\n",
      "finally 225\n",
      "getting 226\n",
      "must 227\n",
      "friend 228\n",
      "introspective 229\n",
      "mission 230\n",
      "head 231\n",
      "imagine 232\n",
      "against 233\n",
      "alternating 234\n",
      "fierce 235\n",
      "room 236\n",
      "rang 237\n",
      "before 238\n",
      "brothers 239\n",
      "employing 240\n",
      "time 241\n",
      "her 242\n",
      "tall 243\n",
      "finds 244\n",
      "â€śthis 245\n",
      "march 246\n",
      "who 247\n",
      "effusive 248\n",
      "upon 249\n",
      "door 250\n",
      "journey 251\n",
      "bohemian 252\n",
      "emotion 253\n",
      "fire 254\n",
      "account 255\n",
      "odessa 256\n",
      "the 257\n",
      "eye 258\n",
      "always 259\n",
      "spoke 260\n",
      "adler 261\n",
      "extraordinary 262\n",
      "heard 263\n",
      "delicately 264\n",
      "tragedy 265\n",
      "know 266\n",
      "a 267\n",
      "mysteries 268\n",
      "veil 269\n",
      "country 270\n",
      "immense 271\n",
      "whole 272\n",
      "seven 273\n",
      "signs 274\n",
      "finely 275\n",
      "every 276\n",
      "sex 277\n",
      "half 278\n",
      "street 279\n",
      "society 280\n",
      "since 281\n",
      "be 282\n",
      "intended 283\n",
      "perfect 284\n",
      "woman 285\n",
      "among 286\n",
      "indicated 287\n",
      "spoken 288\n",
      "marriage 289\n",
      "drifted 290\n",
      "associated 291\n",
      "youâ€ť 292\n",
      "clearing 293\n",
      "keen 294\n",
      "mary 295\n",
      "chuckled 296\n",
      "reigning 297\n",
      "energy 298\n",
      "distracting 299\n",
      "she 300\n",
      "centuries 301\n",
      "again 302\n",
      "holmesâ€ť 303\n",
      "late 304\n",
      "us 305\n",
      "seized 306\n",
      "risen 307\n",
      "clothes 308\n",
      "not 309\n",
      "absorb 310\n",
      "trepoff 311\n",
      "civil 312\n",
      "most 313\n",
      "books 314\n",
      "obviously 315\n",
      "away 316\n",
      "accomplished 317\n",
      "cocaine 318\n",
      "few 319\n",
      "ago 320\n",
      "true 321\n",
      "factor 322\n",
      "motives 323\n",
      "hopeless 324\n",
      "your 325\n",
      "problem 326\n",
      "powers 327\n",
      "attitude 328\n",
      "answered 329\n",
      "crime 330\n",
      "our 331\n",
      "cuts 332\n",
      "behind 333\n",
      "adjusted 334\n",
      "â€śseven 335\n",
      "or 336\n",
      "is 337\n",
      "burned 338\n",
      "remained 339\n",
      "former 340\n",
      "singular 341\n",
      "incidents 342\n",
      "occupied 343\n",
      "canâ€™t 344\n",
      "observe 345\n",
      "too 346\n",
      "companion 347\n",
      "nightâ€ 348\n",
      "eyes 349\n",
      "hands 350\n",
      "of 351\n",
      "chamber 352\n",
      "between 353\n",
      "into 354\n",
      "changed 355\n",
      "lover 356\n",
      "so 357\n",
      "loathed 358\n",
      "chest 359\n",
      "reasoner 360\n",
      "was 361\n",
      "little 362\n",
      "spare 363\n",
      "where 364\n",
      "seen 365\n",
      "sneer 366\n",
      "shown 367\n",
      "very 368\n",
      "see 369\n",
      "murder 370\n",
      "desire 371\n",
      "parallel 372\n",
      "hot 373\n",
      "such 374\n",
      "â€śthen 375\n",
      "incorrigible 376\n",
      "simplicity 377\n",
      "world 378\n",
      "should 379\n",
      "across 380\n",
      "brilliantly 381\n",
      "grit 382\n",
      "that 383\n",
      "lodgings 384\n",
      "now 385\n",
      "softer 386\n",
      "buried 387\n",
      "complete 388\n",
      "do 389\n",
      "gibe 390\n",
      "dark 391\n",
      "had 392\n",
      "strong 393\n",
      "story 394\n",
      "swiftly 395\n",
      "clues 396\n",
      "own 397\n",
      "activity 398\n",
      "mood 399\n",
      "master 400\n",
      "over 401\n",
      "dreadful 402\n",
      "think 403\n",
      "doings 404\n",
      "manner 405\n",
      "stood 406\n",
      "family 407\n",
      "placed 408\n",
      "each 409\n",
      "left 410\n",
      "when 411\n",
      "interests 412\n",
      "intrusions 413\n",
      "then 414\n",
      "dreams 415\n",
      "lived 416\n",
      "even 417\n",
      "firelight 418\n",
      "drawing 419\n",
      "part 420\n",
      "actions 421\n",
      "soul 422\n",
      "clumsy 423\n",
      "police 424\n",
      "at 425\n",
      "created 426\n",
      "other 427\n",
      "wife 428\n",
      "these 429\n",
      "mention 430\n",
      "work 431\n",
      "formerly 432\n",
      "balanced 433\n",
      "patient 434\n",
      "go 435\n",
      "following 436\n",
      "particularly 437\n",
      "me 438\n",
      "still 439\n",
      "establishment 440\n",
      "atkinson 441\n"
     ]
    }
   ],
   "source": [
    "for key,value in tokenizer.word_index.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('â€ť', 1), ('he', 2), ('you', 3), ('with', 4), ('my', 5), ('drug', 6), ('in', 7), ('i', 8), ('it', 9), ('they', 10), ('to', 11), ('â€śmy', 12), ('one', 13), ('from', 14), ('home', 15), ('as', 16), ('all', 17), ('but', 18), ('just', 19), ('out', 20), ('and', 21), ('how', 22), ('study', 23), ('â€śi', 24), ('his', 25), ('vague', 26), ('bell', 27), ('fashion', 28), ('while', 29), ('has', 30), ('deeply', 31), ('introduce', 32), ('saw', 33), ('suits', 34), ('readers', 35), ('did', 36), ('mental', 37), ('â€śwedlock', 38), ('which', 39), ('memory', 40), ('akin', 41), ('summons', 42), ('twice', 43), ('disturbing', 44), ('observing', 45), ('position', 46), ('thursday', 47), ('abhorrent', 48), ('blind', 49), ('â€śit', 50), ('under', 51), ('sensitive', 52), ('daily', 53), ('were', 54), ('admirable', 55), ('throw', 56), ('mind', 57), ('eagerly', 58), ('there', 59), ('take', 60), ('attracted', 61), ('inside', 62), ('results', 63), ('scent', 64), ('sufficient', 65), ('emotions', 66), ('their', 67), ('false', 68), ('up', 69), ('those', 70), ('for', 71), ('baker', 72), ('said', 73), ('trained', 74), ('careless', 75), ('beyond', 76), ('old', 77), ('told', 78), ('holland', 79), ('cold', 80), ('threw', 81), ('rubbed', 82), ('abandoned', 83), ('admit', 84), ('more', 85), ('admirably', 86), ('well', 87), ('remembered', 88), ('precise', 89), ('felt', 90), ('observerâ€', 91), ('faculties', 92), ('happiness', 93), ('however', 94), ('lately', 95), ('drowsiness', 96), ('save', 97), ('love', 98), ('armchair', 99), ('passed', 100), ('word', 101), ('things', 102), ('led', 103), ('temperament', 104), ('would', 105), ('returned', 106), ('hardly', 107), ('observation', 108), ('fail', 109), ('given', 110), ('gasogene', 111), ('crack', 112), ('lit', 113), ('pass', 114), ('on', 115), ('irene', 116), ('never', 117), ('scored', 118), ('been', 119), ('first', 120), ('leather', 121), ('came', 122), ('put', 123), ('thought', 124), ('nervous', 125), ('shared', 126), ('holmes', 127), ('rise', 128), ('yourself', 129), ('week', 130), ('sherlock', 131), ('dubious', 132), ('merely', 133), ('harness', 134), ('almost', 135), ('fancy', 136), ('questionable', 137), ('high', 138), ('power', 139), ('1888â€', 140), ('trifle', 141), ('by', 142), ('have', 143), ('pounds', 144), ('deduce', 145), ('cigars', 146), ('press', 147), ('successfully', 148), ('walk', 149), ('together', 150), ('nature', 151), ('name', 152), ('sunk', 153), ('certainly', 154), ('menâ€™s', 155), ('silhouette', 156), ('returning', 157), ('ambition', 158), ('”', 159), ('wet', 160), ('watson', 161), ('attention', 162), ('than', 163), ('six', 164), ('pacing', 165), ('instrument', 166), ('delicate', 167), ('mess', 168), ('case', 169), ('kindly', 170), ('seldom', 171), ('might', 172), ('notice', 173), ('much', 174), ('lenses', 175), ('doubt', 176), ('scarlet', 177), ('clasped', 178), ('new', 179), ('an', 180), ('around', 181), ('passions', 182), ('form', 183), ('remarked', 184), ('reasoning', 185), ('spirit', 186), ('man', 187), ('way', 188), ('practice', 189), ('predominates', 190), ('long', 191), ('eclipses', 192), ('trincomalee', 193), ('himself', 194), ('through', 195), ('wooing', 196), ('figure', 197), ('shoe', 198), ('any', 199), ('itselfâ€ť', 200), ('girl', 201), ('some', 202), ('machine', 203), ('him', 204), ('â€śindeed', 205), ('yet', 206), ('knew', 207), ('centred', 208), ('dear', 209), ('strikes', 210), ('ever', 211), ('official', 212), ('rooms', 213), ('corner', 214), ('twentieth', 215), ('habit', 216), ('waved', 217), ('glad', 218), ('looked', 219), ('servant', 220), ('jane', 221), ('tell', 222), ('caused', 223), ('excellent', 224), ('finally', 225), ('getting', 226), ('must', 227), ('friend', 228), ('introspective', 229), ('mission', 230), ('head', 231), ('imagine', 232), ('against', 233), ('alternating', 234), ('fierce', 235), ('room', 236), ('rang', 237), ('before', 238), ('brothers', 239), ('employing', 240), ('time', 241), ('her', 242), ('tall', 243), ('finds', 244), ('â€śthis', 245), ('march', 246), ('who', 247), ('effusive', 248), ('upon', 249), ('door', 250), ('journey', 251), ('bohemian', 252), ('emotion', 253), ('fire', 254), ('account', 255), ('odessa', 256), ('the', 257), ('eye', 258), ('always', 259), ('spoke', 260), ('adler', 261), ('extraordinary', 262), ('heard', 263), ('delicately', 264), ('tragedy', 265), ('know', 266), ('a', 267), ('mysteries', 268), ('veil', 269), ('country', 270), ('immense', 271), ('whole', 272), ('seven', 273), ('signs', 274), ('finely', 275), ('every', 276), ('sex', 277), ('half', 278), ('street', 279), ('society', 280), ('since', 281), ('be', 282), ('intended', 283), ('perfect', 284), ('woman', 285), ('among', 286), ('indicated', 287), ('spoken', 288), ('marriage', 289), ('drifted', 290), ('associated', 291), ('youâ€ť', 292), ('clearing', 293), ('keen', 294), ('mary', 295), ('chuckled', 296), ('reigning', 297), ('energy', 298), ('distracting', 299), ('she', 300), ('centuries', 301), ('again', 302), ('holmesâ€ť', 303), ('late', 304), ('us', 305), ('seized', 306), ('risen', 307), ('clothes', 308), ('not', 309), ('absorb', 310), ('trepoff', 311), ('civil', 312), ('most', 313), ('books', 314), ('obviously', 315), ('away', 316), ('accomplished', 317), ('cocaine', 318), ('few', 319), ('ago', 320), ('true', 321), ('factor', 322), ('motives', 323), ('hopeless', 324), ('your', 325), ('problem', 326), ('powers', 327), ('attitude', 328), ('answered', 329), ('crime', 330), ('our', 331), ('cuts', 332), ('behind', 333), ('adjusted', 334), ('â€śseven', 335), ('or', 336), ('is', 337), ('burned', 338), ('remained', 339), ('former', 340), ('singular', 341), ('incidents', 342), ('occupied', 343), ('canâ€™t', 344), ('observe', 345), ('too', 346), ('companion', 347), ('nightâ€', 348), ('eyes', 349), ('hands', 350), ('of', 351), ('chamber', 352), ('between', 353), ('into', 354), ('changed', 355), ('lover', 356), ('so', 357), ('loathed', 358), ('chest', 359), ('reasoner', 360), ('was', 361), ('little', 362), ('spare', 363), ('where', 364), ('seen', 365), ('sneer', 366), ('shown', 367), ('very', 368), ('see', 369), ('murder', 370), ('desire', 371), ('parallel', 372), ('hot', 373), ('such', 374), ('â€śthen', 375), ('incorrigible', 376), ('simplicity', 377), ('world', 378), ('should', 379), ('across', 380), ('brilliantly', 381), ('grit', 382), ('that', 383), ('lodgings', 384), ('now', 385), ('softer', 386), ('buried', 387), ('complete', 388), ('do', 389), ('gibe', 390), ('dark', 391), ('had', 392), ('strong', 393), ('story', 394), ('swiftly', 395), ('clues', 396), ('own', 397), ('activity', 398), ('mood', 399), ('master', 400), ('over', 401), ('dreadful', 402), ('think', 403), ('doings', 404), ('manner', 405), ('stood', 406), ('family', 407), ('placed', 408), ('each', 409), ('left', 410), ('when', 411), ('interests', 412), ('intrusions', 413), ('then', 414), ('dreams', 415), ('lived', 416), ('even', 417), ('firelight', 418), ('drawing', 419), ('part', 420), ('actions', 421), ('soul', 422), ('clumsy', 423), ('police', 424), ('at', 425), ('created', 426), ('other', 427), ('wife', 428), ('these', 429), ('mention', 430), ('work', 431), ('formerly', 432), ('balanced', 433), ('patient', 434), ('go', 435), ('following', 436), ('particularly', 437), ('me', 438), ('still', 439), ('establishment', 440), ('atkinson', 441)])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "slownik = {1:'A', 2:'B', 3:'C', 3:'D'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "slownik2 = {value:key for key, value in slownik.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'B': 2, 'D': 3}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slownik2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
