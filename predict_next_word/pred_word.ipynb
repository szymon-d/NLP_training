{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    sent = nltk.sent_tokenize(text = text)\n",
    "    corpus = ''\n",
    "    for i in sent:\n",
    "        review = re.sub('/s+', ' ', i)\n",
    "        review = re.sub('/d', '', review)\n",
    "        review = re.sub(',', '', review)\n",
    "        tokens = nltk.word_tokenize(review)\n",
    "        tokens = ' '.join(tokens)\n",
    "        corpus += tokens\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = text_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_duplicate(text):\n",
    "    tokens = text.split(' ')\n",
    "    tokens = set(tokens)\n",
    "    corpus = ' '.join(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = del_duplicate(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'immense extraordinary perfect signs do would drifted did room might story Holland some sneer attention dubious finally yet for high-power .It following never merely .â€śWedlock twice risen at wife He passions name scent Holmes fierce emotions there friend mess seven a in throw .Just strikes books .But had six mood out.â€ť chest felt those tell .As ( cocaine nightâ€ too rise centuries door habit â€śThen corner head told true Trepoff deeply careless nature new .Obviously out Jane thought intrusions daily home way left incidents employing is activity marriage on through clasped lived than .â€śI every part brilliantly imagine reigning .Grit things tragedy eye .All been nervous .He man given balanced getting lenses .From our canâ€™t itselfâ€ť notice glad more delicate but Mary abandoned you waved pacing Irene armchair sex as incorrigible your energy adjusted master interests between time pass manner excellent predominates own rubbed certainly you.â€ť .â€śIt singular menâ€™s .To .They journey abhorrent among seized said were saw came murder has dark much heard fashion March dreams brothers answered still wet eagerly must figure soul rooms observe kindly motives stood sufficient again simplicity ago chamber go blind around mysteries country society remarked particularly where questionable hot alternating Bohemian any desire before .In mental they he former happiness an suits gasogene it chuckled softer caused fire civil deduce over machine position me returned the absorb emotion : shoe or .And crack cold shared Scarlet late silhouette almost little few such vague returning police establishment always Watson clothes seldom himself hopeless other well-remembered admirable very placed away how patient clumsy â€ť into strong admirably trifle doings Street home-centred complete â€śSeven occupied ambition loathed spirit summons all cuts eclipses to old should walk Holmesâ€ť lately problem these case first against drug-created Odessa harness.â€ť family .His youâ€ť dear doubt with led crime up Baker disturbing accomplished put Adler knew spoke passed take so clues actions long readers delicately distracting see yourself by observing however reasoner which when lodgings attitude who each drowsiness fancy admit leather â€śI temperament now love false precise mention and observerâ€ woman servant him factor pounds ” faculties together their .â€śIndeed successfully intended work have just know sunk seen whole spare upon lit shown indicated think veil study most .One Atkinson associated swiftly mind keen inside ; â€śMy under observation week mission introspective fail remained buried ) even .How dreadful instrument attracted Trincomalee effusive .Then 1888â€ ! results was wooing practice scored finely bell while world finds parallel memory drawing .I not press Sherlock cigars lover Study behind reasoning sensitive across spoken his .Beyond girl akin ? I gibe rang since ever looked official tall powers us firelight formerly drug changed burned my hardly word trained eyes form twentieth save half one from of that â€śmy be Thursday .You .My hands she clearing introduce .With companion threw â€śthis her account'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "features_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text_3])\n",
    "\n",
    "tokenizer_rep = tokenizer.texts_to_sequences([text_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data):\n",
    "    data = np.squeeze(np.array(data))\n",
    "    X_len = 3\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i > len(data) - X_len - 1:\n",
    "            continue\n",
    "        else:\n",
    "            X.append(data[i:i + X_len])\n",
    "            y.append(data[i + X_len])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_split(tokenizer_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_weights.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, features_num, input_length = 3))\n",
    "model.add(LSTM(units = 1000, return_sequences = True))\n",
    "model.add(LSTM(units = 1000))\n",
    "model.add(Dense(units = 1000, activation = 'relu'))\n",
    "model.add(Dense(units = vocab_size, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3, 10)             10000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 3, 1000)           4044000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000)              8004000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1001000   \n",
      "=================================================================\n",
      "Total params: 14,060,000\n",
      "Trainable params: 14,060,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 26s 2s/step - loss: 6.9085\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.90932, saving model to best_weights.h5\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 6s 745ms/step - loss: 6.8876\n",
      "\n",
      "Epoch 00002: loss improved from 6.90932 to 6.87814, saving model to best_weights.h5\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 6s 751ms/step - loss: 6.5907\n",
      "\n",
      "Epoch 00003: loss improved from 6.87814 to 6.60811, saving model to best_weights.h5\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 9s 1s/step - loss: 6.2767\n",
      "\n",
      "Epoch 00004: loss improved from 6.60811 to 6.30399, saving model to best_weights.h5\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 8s 939ms/step - loss: 6.1839\n",
      "\n",
      "Epoch 00005: loss improved from 6.30399 to 6.21289, saving model to best_weights.h5\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 7s 880ms/step - loss: 6.1467\n",
      "\n",
      "Epoch 00006: loss improved from 6.21289 to 6.15280, saving model to best_weights.h5\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 7s 876ms/step - loss: 6.1225\n",
      "\n",
      "Epoch 00007: loss improved from 6.15280 to 6.12699, saving model to best_weights.h5\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 7s 890ms/step - loss: 6.0885\n",
      "\n",
      "Epoch 00008: loss improved from 6.12699 to 6.11995, saving model to best_weights.h5\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 7s 922ms/step - loss: 6.0709\n",
      "\n",
      "Epoch 00009: loss improved from 6.11995 to 6.09792, saving model to best_weights.h5\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 7s 919ms/step - loss: 6.0589\n",
      "\n",
      "Epoch 00010: loss improved from 6.09792 to 6.08293, saving model to best_weights.h5\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 7s 908ms/step - loss: 6.0392\n",
      "\n",
      "Epoch 00011: loss improved from 6.08293 to 6.05304, saving model to best_weights.h5\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 7s 899ms/step - loss: 6.0194\n",
      "\n",
      "Epoch 00012: loss improved from 6.05304 to 6.01945, saving model to best_weights.h5\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 7s 890ms/step - loss: 5.9390\n",
      "\n",
      "Epoch 00013: loss improved from 6.01945 to 5.97147, saving model to best_weights.h5\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 7s 880ms/step - loss: 5.9175\n",
      "\n",
      "Epoch 00014: loss improved from 5.97147 to 5.93838, saving model to best_weights.h5\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 7s 897ms/step - loss: 5.8972\n",
      "\n",
      "Epoch 00015: loss improved from 5.93838 to 5.90560, saving model to best_weights.h5\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 7s 870ms/step - loss: 5.8842\n",
      "\n",
      "Epoch 00016: loss improved from 5.90560 to 5.88772, saving model to best_weights.h5\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 7s 918ms/step - loss: 5.7938\n",
      "\n",
      "Epoch 00017: loss improved from 5.88772 to 5.82813, saving model to best_weights.h5\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 7s 894ms/step - loss: 5.6874\n",
      "\n",
      "Epoch 00018: loss improved from 5.82813 to 5.73626, saving model to best_weights.h5\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 7s 867ms/step - loss: 5.5932\n",
      "\n",
      "Epoch 00019: loss improved from 5.73626 to 5.61628, saving model to best_weights.h5\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 7s 876ms/step - loss: 5.5209\n",
      "\n",
      "Epoch 00020: loss improved from 5.61628 to 5.56462, saving model to best_weights.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2595e603ee0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 20, batch_size = 64, \n",
    "         callbacks = [checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for key in tokenizer.word_index.keys():\n",
    "    word_index.update({tokenizer.word_index[key]: key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokenized_text = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    y_pred = model.predict(tokenized_text)\n",
    "    y_pred = y_pred.argmax()\n",
    "    \n",
    "    word = word_index[y_pred]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Why','have', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    to_pred = ' '.join(text[len(text)-3:])\n",
    "    word = predict(to_pred)\n",
    "    text.append(word)\n",
    "\n",
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why have you over loathed â€śi police part journey created itselfâ€ť dreadful excellent high six did away came answered rooms saw â€śi akin be true true harness from from home holmes holmes strikes official few admit â€śi true introspective sherlock home canâ€™t books'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€ť 1\n",
      "he 2\n",
      "you 3\n",
      "with 4\n",
      "my 5\n",
      "drug 6\n",
      "in 7\n",
      "i 8\n",
      "it 9\n",
      "they 10\n",
      "to 11\n",
      "â€śmy 12\n",
      "one 13\n",
      "from 14\n",
      "home 15\n",
      "as 16\n",
      "all 17\n",
      "but 18\n",
      "just 19\n",
      "out 20\n",
      "and 21\n",
      "how 22\n",
      "study 23\n",
      "â€śi 24\n",
      "his 25\n",
      "vague 26\n",
      "bell 27\n",
      "fashion 28\n",
      "while 29\n",
      "has 30\n",
      "deeply 31\n",
      "introduce 32\n",
      "saw 33\n",
      "suits 34\n",
      "readers 35\n",
      "did 36\n",
      "mental 37\n",
      "â€śwedlock 38\n",
      "which 39\n",
      "memory 40\n",
      "akin 41\n",
      "summons 42\n",
      "twice 43\n",
      "disturbing 44\n",
      "observing 45\n",
      "position 46\n",
      "thursday 47\n",
      "abhorrent 48\n",
      "blind 49\n",
      "â€śit 50\n",
      "under 51\n",
      "sensitive 52\n",
      "daily 53\n",
      "were 54\n",
      "admirable 55\n",
      "throw 56\n",
      "mind 57\n",
      "eagerly 58\n",
      "there 59\n",
      "take 60\n",
      "attracted 61\n",
      "inside 62\n",
      "results 63\n",
      "scent 64\n",
      "sufficient 65\n",
      "emotions 66\n",
      "their 67\n",
      "false 68\n",
      "up 69\n",
      "those 70\n",
      "for 71\n",
      "baker 72\n",
      "said 73\n",
      "trained 74\n",
      "careless 75\n",
      "beyond 76\n",
      "old 77\n",
      "told 78\n",
      "holland 79\n",
      "cold 80\n",
      "threw 81\n",
      "rubbed 82\n",
      "abandoned 83\n",
      "admit 84\n",
      "more 85\n",
      "admirably 86\n",
      "well 87\n",
      "remembered 88\n",
      "precise 89\n",
      "felt 90\n",
      "observerâ€ 91\n",
      "faculties 92\n",
      "happiness 93\n",
      "however 94\n",
      "lately 95\n",
      "drowsiness 96\n",
      "save 97\n",
      "love 98\n",
      "armchair 99\n",
      "passed 100\n",
      "word 101\n",
      "things 102\n",
      "led 103\n",
      "temperament 104\n",
      "would 105\n",
      "returned 106\n",
      "hardly 107\n",
      "observation 108\n",
      "fail 109\n",
      "given 110\n",
      "gasogene 111\n",
      "crack 112\n",
      "lit 113\n",
      "pass 114\n",
      "on 115\n",
      "irene 116\n",
      "never 117\n",
      "scored 118\n",
      "been 119\n",
      "first 120\n",
      "leather 121\n",
      "came 122\n",
      "put 123\n",
      "thought 124\n",
      "nervous 125\n",
      "shared 126\n",
      "holmes 127\n",
      "rise 128\n",
      "yourself 129\n",
      "week 130\n",
      "sherlock 131\n",
      "dubious 132\n",
      "merely 133\n",
      "harness 134\n",
      "almost 135\n",
      "fancy 136\n",
      "questionable 137\n",
      "high 138\n",
      "power 139\n",
      "1888â€ 140\n",
      "trifle 141\n",
      "by 142\n",
      "have 143\n",
      "pounds 144\n",
      "deduce 145\n",
      "cigars 146\n",
      "press 147\n",
      "successfully 148\n",
      "walk 149\n",
      "together 150\n",
      "nature 151\n",
      "name 152\n",
      "sunk 153\n",
      "certainly 154\n",
      "menâ€™s 155\n",
      "silhouette 156\n",
      "returning 157\n",
      "ambition 158\n",
      "” 159\n",
      "wet 160\n",
      "watson 161\n",
      "attention 162\n",
      "than 163\n",
      "six 164\n",
      "pacing 165\n",
      "instrument 166\n",
      "delicate 167\n",
      "mess 168\n",
      "case 169\n",
      "kindly 170\n",
      "seldom 171\n",
      "might 172\n",
      "notice 173\n",
      "much 174\n",
      "lenses 175\n",
      "doubt 176\n",
      "scarlet 177\n",
      "clasped 178\n",
      "new 179\n",
      "an 180\n",
      "around 181\n",
      "passions 182\n",
      "form 183\n",
      "remarked 184\n",
      "reasoning 185\n",
      "spirit 186\n",
      "man 187\n",
      "way 188\n",
      "practice 189\n",
      "predominates 190\n",
      "long 191\n",
      "eclipses 192\n",
      "trincomalee 193\n",
      "himself 194\n",
      "through 195\n",
      "wooing 196\n",
      "figure 197\n",
      "shoe 198\n",
      "any 199\n",
      "itselfâ€ť 200\n",
      "girl 201\n",
      "some 202\n",
      "machine 203\n",
      "him 204\n",
      "â€śindeed 205\n",
      "yet 206\n",
      "knew 207\n",
      "centred 208\n",
      "dear 209\n",
      "strikes 210\n",
      "ever 211\n",
      "official 212\n",
      "rooms 213\n",
      "corner 214\n",
      "twentieth 215\n",
      "habit 216\n",
      "waved 217\n",
      "glad 218\n",
      "looked 219\n",
      "servant 220\n",
      "jane 221\n",
      "tell 222\n",
      "caused 223\n",
      "excellent 224\n",
      "finally 225\n",
      "getting 226\n",
      "must 227\n",
      "friend 228\n",
      "introspective 229\n",
      "mission 230\n",
      "head 231\n",
      "imagine 232\n",
      "against 233\n",
      "alternating 234\n",
      "fierce 235\n",
      "room 236\n",
      "rang 237\n",
      "before 238\n",
      "brothers 239\n",
      "employing 240\n",
      "time 241\n",
      "her 242\n",
      "tall 243\n",
      "finds 244\n",
      "â€śthis 245\n",
      "march 246\n",
      "who 247\n",
      "effusive 248\n",
      "upon 249\n",
      "door 250\n",
      "journey 251\n",
      "bohemian 252\n",
      "emotion 253\n",
      "fire 254\n",
      "account 255\n",
      "odessa 256\n",
      "the 257\n",
      "eye 258\n",
      "always 259\n",
      "spoke 260\n",
      "adler 261\n",
      "extraordinary 262\n",
      "heard 263\n",
      "delicately 264\n",
      "tragedy 265\n",
      "know 266\n",
      "a 267\n",
      "mysteries 268\n",
      "veil 269\n",
      "country 270\n",
      "immense 271\n",
      "whole 272\n",
      "seven 273\n",
      "signs 274\n",
      "finely 275\n",
      "every 276\n",
      "sex 277\n",
      "half 278\n",
      "street 279\n",
      "society 280\n",
      "since 281\n",
      "be 282\n",
      "intended 283\n",
      "perfect 284\n",
      "woman 285\n",
      "among 286\n",
      "indicated 287\n",
      "spoken 288\n",
      "marriage 289\n",
      "drifted 290\n",
      "associated 291\n",
      "youâ€ť 292\n",
      "clearing 293\n",
      "keen 294\n",
      "mary 295\n",
      "chuckled 296\n",
      "reigning 297\n",
      "energy 298\n",
      "distracting 299\n",
      "she 300\n",
      "centuries 301\n",
      "again 302\n",
      "holmesâ€ť 303\n",
      "late 304\n",
      "us 305\n",
      "seized 306\n",
      "risen 307\n",
      "clothes 308\n",
      "not 309\n",
      "absorb 310\n",
      "trepoff 311\n",
      "civil 312\n",
      "most 313\n",
      "books 314\n",
      "obviously 315\n",
      "away 316\n",
      "accomplished 317\n",
      "cocaine 318\n",
      "few 319\n",
      "ago 320\n",
      "true 321\n",
      "factor 322\n",
      "motives 323\n",
      "hopeless 324\n",
      "your 325\n",
      "problem 326\n",
      "powers 327\n",
      "attitude 328\n",
      "answered 329\n",
      "crime 330\n",
      "our 331\n",
      "cuts 332\n",
      "behind 333\n",
      "adjusted 334\n",
      "â€śseven 335\n",
      "or 336\n",
      "is 337\n",
      "burned 338\n",
      "remained 339\n",
      "former 340\n",
      "singular 341\n",
      "incidents 342\n",
      "occupied 343\n",
      "canâ€™t 344\n",
      "observe 345\n",
      "too 346\n",
      "companion 347\n",
      "nightâ€ 348\n",
      "eyes 349\n",
      "hands 350\n",
      "of 351\n",
      "chamber 352\n",
      "between 353\n",
      "into 354\n",
      "changed 355\n",
      "lover 356\n",
      "so 357\n",
      "loathed 358\n",
      "chest 359\n",
      "reasoner 360\n",
      "was 361\n",
      "little 362\n",
      "spare 363\n",
      "where 364\n",
      "seen 365\n",
      "sneer 366\n",
      "shown 367\n",
      "very 368\n",
      "see 369\n",
      "murder 370\n",
      "desire 371\n",
      "parallel 372\n",
      "hot 373\n",
      "such 374\n",
      "â€śthen 375\n",
      "incorrigible 376\n",
      "simplicity 377\n",
      "world 378\n",
      "should 379\n",
      "across 380\n",
      "brilliantly 381\n",
      "grit 382\n",
      "that 383\n",
      "lodgings 384\n",
      "now 385\n",
      "softer 386\n",
      "buried 387\n",
      "complete 388\n",
      "do 389\n",
      "gibe 390\n",
      "dark 391\n",
      "had 392\n",
      "strong 393\n",
      "story 394\n",
      "swiftly 395\n",
      "clues 396\n",
      "own 397\n",
      "activity 398\n",
      "mood 399\n",
      "master 400\n",
      "over 401\n",
      "dreadful 402\n",
      "think 403\n",
      "doings 404\n",
      "manner 405\n",
      "stood 406\n",
      "family 407\n",
      "placed 408\n",
      "each 409\n",
      "left 410\n",
      "when 411\n",
      "interests 412\n",
      "intrusions 413\n",
      "then 414\n",
      "dreams 415\n",
      "lived 416\n",
      "even 417\n",
      "firelight 418\n",
      "drawing 419\n",
      "part 420\n",
      "actions 421\n",
      "soul 422\n",
      "clumsy 423\n",
      "police 424\n",
      "at 425\n",
      "created 426\n",
      "other 427\n",
      "wife 428\n",
      "these 429\n",
      "mention 430\n",
      "work 431\n",
      "formerly 432\n",
      "balanced 433\n",
      "patient 434\n",
      "go 435\n",
      "following 436\n",
      "particularly 437\n",
      "me 438\n",
      "still 439\n",
      "establishment 440\n",
      "atkinson 441\n"
     ]
    }
   ],
   "source": [
    "for key,value in tokenizer.word_index.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('â€ť', 1), ('he', 2), ('you', 3), ('with', 4), ('my', 5), ('drug', 6), ('in', 7), ('i', 8), ('it', 9), ('they', 10), ('to', 11), ('â€śmy', 12), ('one', 13), ('from', 14), ('home', 15), ('as', 16), ('all', 17), ('but', 18), ('just', 19), ('out', 20), ('and', 21), ('how', 22), ('study', 23), ('â€śi', 24), ('his', 25), ('vague', 26), ('bell', 27), ('fashion', 28), ('while', 29), ('has', 30), ('deeply', 31), ('introduce', 32), ('saw', 33), ('suits', 34), ('readers', 35), ('did', 36), ('mental', 37), ('â€śwedlock', 38), ('which', 39), ('memory', 40), ('akin', 41), ('summons', 42), ('twice', 43), ('disturbing', 44), ('observing', 45), ('position', 46), ('thursday', 47), ('abhorrent', 48), ('blind', 49), ('â€śit', 50), ('under', 51), ('sensitive', 52), ('daily', 53), ('were', 54), ('admirable', 55), ('throw', 56), ('mind', 57), ('eagerly', 58), ('there', 59), ('take', 60), ('attracted', 61), ('inside', 62), ('results', 63), ('scent', 64), ('sufficient', 65), ('emotions', 66), ('their', 67), ('false', 68), ('up', 69), ('those', 70), ('for', 71), ('baker', 72), ('said', 73), ('trained', 74), ('careless', 75), ('beyond', 76), ('old', 77), ('told', 78), ('holland', 79), ('cold', 80), ('threw', 81), ('rubbed', 82), ('abandoned', 83), ('admit', 84), ('more', 85), ('admirably', 86), ('well', 87), ('remembered', 88), ('precise', 89), ('felt', 90), ('observerâ€', 91), ('faculties', 92), ('happiness', 93), ('however', 94), ('lately', 95), ('drowsiness', 96), ('save', 97), ('love', 98), ('armchair', 99), ('passed', 100), ('word', 101), ('things', 102), ('led', 103), ('temperament', 104), ('would', 105), ('returned', 106), ('hardly', 107), ('observation', 108), ('fail', 109), ('given', 110), ('gasogene', 111), ('crack', 112), ('lit', 113), ('pass', 114), ('on', 115), ('irene', 116), ('never', 117), ('scored', 118), ('been', 119), ('first', 120), ('leather', 121), ('came', 122), ('put', 123), ('thought', 124), ('nervous', 125), ('shared', 126), ('holmes', 127), ('rise', 128), ('yourself', 129), ('week', 130), ('sherlock', 131), ('dubious', 132), ('merely', 133), ('harness', 134), ('almost', 135), ('fancy', 136), ('questionable', 137), ('high', 138), ('power', 139), ('1888â€', 140), ('trifle', 141), ('by', 142), ('have', 143), ('pounds', 144), ('deduce', 145), ('cigars', 146), ('press', 147), ('successfully', 148), ('walk', 149), ('together', 150), ('nature', 151), ('name', 152), ('sunk', 153), ('certainly', 154), ('menâ€™s', 155), ('silhouette', 156), ('returning', 157), ('ambition', 158), ('”', 159), ('wet', 160), ('watson', 161), ('attention', 162), ('than', 163), ('six', 164), ('pacing', 165), ('instrument', 166), ('delicate', 167), ('mess', 168), ('case', 169), ('kindly', 170), ('seldom', 171), ('might', 172), ('notice', 173), ('much', 174), ('lenses', 175), ('doubt', 176), ('scarlet', 177), ('clasped', 178), ('new', 179), ('an', 180), ('around', 181), ('passions', 182), ('form', 183), ('remarked', 184), ('reasoning', 185), ('spirit', 186), ('man', 187), ('way', 188), ('practice', 189), ('predominates', 190), ('long', 191), ('eclipses', 192), ('trincomalee', 193), ('himself', 194), ('through', 195), ('wooing', 196), ('figure', 197), ('shoe', 198), ('any', 199), ('itselfâ€ť', 200), ('girl', 201), ('some', 202), ('machine', 203), ('him', 204), ('â€śindeed', 205), ('yet', 206), ('knew', 207), ('centred', 208), ('dear', 209), ('strikes', 210), ('ever', 211), ('official', 212), ('rooms', 213), ('corner', 214), ('twentieth', 215), ('habit', 216), ('waved', 217), ('glad', 218), ('looked', 219), ('servant', 220), ('jane', 221), ('tell', 222), ('caused', 223), ('excellent', 224), ('finally', 225), ('getting', 226), ('must', 227), ('friend', 228), ('introspective', 229), ('mission', 230), ('head', 231), ('imagine', 232), ('against', 233), ('alternating', 234), ('fierce', 235), ('room', 236), ('rang', 237), ('before', 238), ('brothers', 239), ('employing', 240), ('time', 241), ('her', 242), ('tall', 243), ('finds', 244), ('â€śthis', 245), ('march', 246), ('who', 247), ('effusive', 248), ('upon', 249), ('door', 250), ('journey', 251), ('bohemian', 252), ('emotion', 253), ('fire', 254), ('account', 255), ('odessa', 256), ('the', 257), ('eye', 258), ('always', 259), ('spoke', 260), ('adler', 261), ('extraordinary', 262), ('heard', 263), ('delicately', 264), ('tragedy', 265), ('know', 266), ('a', 267), ('mysteries', 268), ('veil', 269), ('country', 270), ('immense', 271), ('whole', 272), ('seven', 273), ('signs', 274), ('finely', 275), ('every', 276), ('sex', 277), ('half', 278), ('street', 279), ('society', 280), ('since', 281), ('be', 282), ('intended', 283), ('perfect', 284), ('woman', 285), ('among', 286), ('indicated', 287), ('spoken', 288), ('marriage', 289), ('drifted', 290), ('associated', 291), ('youâ€ť', 292), ('clearing', 293), ('keen', 294), ('mary', 295), ('chuckled', 296), ('reigning', 297), ('energy', 298), ('distracting', 299), ('she', 300), ('centuries', 301), ('again', 302), ('holmesâ€ť', 303), ('late', 304), ('us', 305), ('seized', 306), ('risen', 307), ('clothes', 308), ('not', 309), ('absorb', 310), ('trepoff', 311), ('civil', 312), ('most', 313), ('books', 314), ('obviously', 315), ('away', 316), ('accomplished', 317), ('cocaine', 318), ('few', 319), ('ago', 320), ('true', 321), ('factor', 322), ('motives', 323), ('hopeless', 324), ('your', 325), ('problem', 326), ('powers', 327), ('attitude', 328), ('answered', 329), ('crime', 330), ('our', 331), ('cuts', 332), ('behind', 333), ('adjusted', 334), ('â€śseven', 335), ('or', 336), ('is', 337), ('burned', 338), ('remained', 339), ('former', 340), ('singular', 341), ('incidents', 342), ('occupied', 343), ('canâ€™t', 344), ('observe', 345), ('too', 346), ('companion', 347), ('nightâ€', 348), ('eyes', 349), ('hands', 350), ('of', 351), ('chamber', 352), ('between', 353), ('into', 354), ('changed', 355), ('lover', 356), ('so', 357), ('loathed', 358), ('chest', 359), ('reasoner', 360), ('was', 361), ('little', 362), ('spare', 363), ('where', 364), ('seen', 365), ('sneer', 366), ('shown', 367), ('very', 368), ('see', 369), ('murder', 370), ('desire', 371), ('parallel', 372), ('hot', 373), ('such', 374), ('â€śthen', 375), ('incorrigible', 376), ('simplicity', 377), ('world', 378), ('should', 379), ('across', 380), ('brilliantly', 381), ('grit', 382), ('that', 383), ('lodgings', 384), ('now', 385), ('softer', 386), ('buried', 387), ('complete', 388), ('do', 389), ('gibe', 390), ('dark', 391), ('had', 392), ('strong', 393), ('story', 394), ('swiftly', 395), ('clues', 396), ('own', 397), ('activity', 398), ('mood', 399), ('master', 400), ('over', 401), ('dreadful', 402), ('think', 403), ('doings', 404), ('manner', 405), ('stood', 406), ('family', 407), ('placed', 408), ('each', 409), ('left', 410), ('when', 411), ('interests', 412), ('intrusions', 413), ('then', 414), ('dreams', 415), ('lived', 416), ('even', 417), ('firelight', 418), ('drawing', 419), ('part', 420), ('actions', 421), ('soul', 422), ('clumsy', 423), ('police', 424), ('at', 425), ('created', 426), ('other', 427), ('wife', 428), ('these', 429), ('mention', 430), ('work', 431), ('formerly', 432), ('balanced', 433), ('patient', 434), ('go', 435), ('following', 436), ('particularly', 437), ('me', 438), ('still', 439), ('establishment', 440), ('atkinson', 441)])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "slownik = {1:'A', 2:'B', 3:'C', 3:'D'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "slownik2 = {value:key for key, value in slownik.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'B': 2, 'D': 3}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slownik2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
